# C.U.R.I.E. - Clever Understanding and Reasoning Intelligent Entity

Curie is an AI assistant that runs **locally** and interacts with users via Telegram.  
It is inspired by conversational assistants like Jarvis from Iron Man, but runs fully on your hardware using state-of-the-art open local language models (no OpenAI account required).

---

## Features

- **Conversational AI** via Telegram
- **Local LLMs**: Runs Meta Llama 3/3.1 or other GGUF models (no cloud needed)
- **Configurable Persona**: Customizable assistant personality via JSON
- Simple to set up and extend

---

## Requirements

- Python 3.8 or higher
- `python-telegram-bot` (v20+)
- `llama-cpp-python`
- `python-dotenv`
- At least one GGUF language model (see below)

---

## Setup

### 1. **Clone the repository**
```sh
git clone https://github.com/yessur3808/curie00.git
cd curie00
```


### 2. **Install dependencies**

```sh
pip install -r requirements.txt
```


### 3. **Download a GGUF LLM model**

- Recommended: Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf
- Place the .gguf file in the models/ directory.


### 4. **Create a `.env` file in the project root**

```env
TELEGRAM_BOT_TOKEN=your_telegram_token
LLM_MODELS=Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf
```
You can list multiple GGUF model files (comma-separated) if you want to support switching later.


### 5. **Set up your persona (optional)**

Edit assets/persona.json to customize the assistantâ€™s name, greeting, and style.


## Running the Bot

```sh
python main.py
```

or

```sh
python3 main.py
```



## Usage

- Start the bot by sending `/start` in your Telegram chat with the bot.
- Send any message to receive a response generated by the local AI model.
- Persona and system prompt are automatically prepended to every message.



## Directory Structure

```
assets/          # Persona JSON files, images
connectors/      # Telegram connector
models/          # Place GGUF models here
scripts/         # Helper scripts
agent.py         # Agent logic
config.py        # Config loader
llm_manager.py   # LLM interface
main.py          # Program entry point
requirements.txt # Dependencies
```



## Notes

- No OpenAI API is used; all LLM inference runs locally.
- For best results, use a quantized 8B model and a machine with at least 8GB RAM.
- To use a different model, update the LLM_MODELS entry in .env.





## Roadmap
- [ ] Add memory and context between messages
- [ ] Support for other chat platforms (Discord, Slack, etc)
- [ ] Web-based interface


---



***Curie is private, local, and customizable AI for everyone!***



Let me know if you want to add installation instructions for **virtual environments**, or extra usage examples!