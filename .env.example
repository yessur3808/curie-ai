TELEGRAM_BOT_TOKEN=your_telegram_bot_token

LLM_MODELS=Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf,Meta-Llama-3-70B-Instruct-Q4_K_M.gguf
CODING_MODEL_NAME=codellama-34b-instruct.Q4_K_M.gguf

# LLM Context Window Configuration
# LLM_CONTEXT_SIZE=2048  # Total context window size (default: 2048)
# LLM_CONTEXT_BUFFER=16  # Buffer reserved for system tokens (default: 16)
# LLM_MIN_TOKENS=64  # Minimum tokens required for a response (default: 64)
# LLM_FALLBACK_MAX_TOKENS=512  # Fallback max_tokens if tokenization fails (default: 512)

# Info Search Task Configuration
# INFO_SEARCH_TEMPERATURE=0.2  # Temperature for info search LLM calls (default: 0.2, lower for more deterministic results)
# INFO_SEARCH_MAX_TOKENS=512  # Max tokens for info search responses (default: 512, must leave room for prompt in 2048 context)
# INFO_SEARCH_MAX_SOURCES=3  # Maximum number of sources to process (default: 3, prevents context overflow)
# INFO_SEARCH_MAX_SNIPPET_CHARS=400  # Maximum characters per snippet (default: 400, ~100 tokens)

MASTER_USER_ID=123456789  # <--- Replace with your own user ID

POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_DB=assistant_db
POSTGRES_USER=your_pg_user
POSTGRES_PASSWORD=your_pg_password

MONGODB_URI=mongodb://localhost:27017/
MONGODB_DB=assistant_db

GITHUB_TOKEN=ghp_*********************************
MAIN_REPO=https://github.com/johndoe/abc_project
MAIN_REVIEWER=MainCoder_or_yourself

RUN_TELEGRAM=true
RUN_API=true
RUN_CODER=false

DEFAULT_PERSONA_NAME=jarvis
DEFAULT_PERSONA=personality.json


PROJECTS_ROOT=/var/projects